{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e696d0e",
   "metadata": {},
   "source": [
    "**Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import vllm.model_executor.layers.logits_processor as LPmod\n",
    "from vllm.model_executor.sampling_metadata import SamplingMetadata\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "from rich.console import Console\n",
    "from rich.text import Text\n",
    "\n",
    "# 设置环境变量\n",
    "\n",
    "import os\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "# 设置常量\n",
    "\n",
    "ADD_GENERATION_PROMPT = True\n",
    "\n",
    "# 导入配置\n",
    "\n",
    "config = json.load(open('../config.json', 'r'))\n",
    "\n",
    "# 定义函数\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "\n",
    "# 统计函数：四分位距\n",
    "def iqr(x: list[float]):\n",
    "    return np.percentile(x, 75) - np.percentile(x, 25)\n",
    "\n",
    "def select_next(stats: Series):\n",
    "    \"\"\"\n",
    "    从统计结果中筛选出：\n",
    "    - 数据长度最长的组 (count 最大)\n",
    "    - 平均值最高的组 (mean 最大)\n",
    "    - 集中度最高的组 (iqr 最小)\n",
    "    返回这些组索引组成的集合\n",
    "    \"\"\"\n",
    "    idx_len = stats[\"count\"].idxmax()        # 数据最长\n",
    "    idx_mean = stats[\"mean\"].idxmax()        # 平均值最高\n",
    "    idx_concentration = stats[\"iqr\"].idxmin()  # 集中度最高（std 最小）\n",
    "\n",
    "    return {idx_len, idx_mean, idx_concentration}\n",
    "\n",
    "# 导入数据集\n",
    "dataset_path = config['dataset']\n",
    "result_path = config['result']\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    questions: list[str] = [json.loads(line)['question'] for line in lines][0:4]\n",
    "    answers: list[str] = [json.loads(line)['answer'] for line in lines][0:4]\n",
    "\n",
    "final_answers = [int(answer.split('#### ')[-1]) for answer in answers]\n",
    "dataset = list(zip(questions, final_answers))\n",
    "\n",
    "\n",
    "# 加载目标模型\n",
    "from vllm import LLM, SamplingParams, RequestOutput\n",
    "\n",
    "target_model = LLM(\n",
    "    model=config['models'][config['model']],\n",
    "    task=\"generate\",\n",
    "    enable_chunked_prefill=False,\n",
    "    max_model_len=config['max_model_len'],\n",
    "    gpu_memory_utilization=config['gpu_memory_utilization']\n",
    ")\n",
    "\n",
    "target_tokenizer = target_model.get_tokenizer()\n",
    "\n",
    "# 加载奖励模型\n",
    "\n",
    "reward_model = LLM(\n",
    "    model=\"/root/project/models/Qwen2.5-Math-7B-PRM800K\",\n",
    "    task=\"reward\",\n",
    "    max_model_len=8192,\n",
    "    enable_chunked_prefill=False,\n",
    "    gpu_memory_utilization=0.25\n",
    ")\n",
    "\n",
    "reward_tokenizer = reward_model.get_tokenizer()\n",
    "\n",
    "# 设置采样参数\n",
    "# 第一阶段采样\n",
    "\n",
    "generating_params_1 = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    stop = [\"\\n\\n\"],\n",
    "    n=10\n",
    ")\n",
    "\n",
    "# 第二阶段采样\n",
    "\n",
    "generating_params_2 = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a860c42",
   "metadata": {},
   "source": [
    "0. 奖励分数统计优化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62940f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question, answer in dataset:\n",
    "    print(f'Question: {question}')\n",
    "\n",
    "    input_text = question + f\"{str(answer)} is wrong.\" # 改写问题\n",
    "    inputs = [target_tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                'role':'user',\n",
    "                'content':content\n",
    "            }\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=ADD_GENERATION_PROMPT\n",
    "    ) for content in input_text]\n",
    "\n",
    "    print(f\"Counterfactual: {input_text}\")\n",
    "\n",
    "    # 迭代优化\n",
    "    # 1. 采样模型回答\n",
    "    # 2. 统计奖励得分\n",
    "    # 3. 选择下一轮输入\n",
    "\n",
    "\n",
    "    gen_of_thinking = 0\n",
    "    flag = True\n",
    "    while(flag and gen_of_thinking<50): # 终止条件：产生循环思考或超过一定轮次\n",
    "\n",
    "        print(f'Generation: {gen_of_thinking}')\n",
    "\n",
    "        # 采样目标模型回答\n",
    "        outputs: list[RequestOutput] = target_model.generate(inputs, generating_params_2)\n",
    "\n",
    "        # 用奖励模型为一批样本的每步思考打分\n",
    "        thinkings: list[str] = []\n",
    "        prompts: list[str] = []\n",
    "        thinking_length: list[int] = []\n",
    "\n",
    "        for i, output in enumerate(outputs):\n",
    "            print(f\"Input: {input_text[i]}\")\n",
    "            for thinking in output.outputs:\n",
    "                text = thinking.text\n",
    "                thinking_length.append(len(thinking.token_ids))\n",
    "                thinkings.append(text)\n",
    "                steps = text.split('\\n\\n')\n",
    "\n",
    "                # 若有输出达到最大模型长度，且重复思考步骤，则终止优化，记录输出\n",
    "                total_token_num = len(target_tokenizer.encode(inputs[i])) + len(thinking.token_ids)\n",
    "                print(f\"token 总数: {total_token_num}\")\n",
    "                if total_token_num >= 4096:\n",
    "                    print(f\"Last 10 steps of a long thinking:\\n\\n{'\\n\\n'.join(steps[-10:])}\")\n",
    "                    if text.count(steps[-2]) > 1:\n",
    "                        flag = False\n",
    "\n",
    "                messages = [\n",
    "                    {\"role\": \"user\", \"content\": input_text[i]},\n",
    "                    {\"role\": \"assistant\", \"content\": \"<extra_0>\".join(steps) + \"<extra_0>\"},\n",
    "                ]\n",
    "\n",
    "                # 输入截断\n",
    "                token_ids = reward_tokenizer.encode(\n",
    "                    reward_tokenizer.apply_chat_template(\n",
    "                        messages, \n",
    "                        tokenize=False\n",
    "                    ),\n",
    "                    truncation=True,\n",
    "                    max_length=4096,\n",
    "                    add_special_tokens=False\n",
    "                )\n",
    "\n",
    "                prompt = reward_tokenizer.decode(token_ids)\n",
    "                pos = prompt.rfind(\"<extra_0>\")\n",
    "                prompts.append(prompt[:pos+9])\n",
    "\n",
    "        rewards = reward_model.encode(prompts)\n",
    "\n",
    "        # 对奖励得分结果进行统计\n",
    "        scores = [reward.outputs.data.cpu().numpy()[:,1] for reward in rewards]\n",
    "        df = pd.DataFrame([\n",
    "            {\"thinking\": i, \"reward\": v}\n",
    "            for i, arr in enumerate(scores)\n",
    "            for v in arr\n",
    "        ])\n",
    "\n",
    "        # 选择进行迭代的样本\n",
    "        # 筛选标准：\n",
    "        # 1. 思考长度\n",
    "        # 2. 平均得分\n",
    "        # 3. 得分集中度\n",
    "        stats = df.groupby(\"thinking\")[\"reward\"].agg(mean=\"mean\")\n",
    "        stats[\"count\"] = thinking_length\n",
    "        stats[\"iqr\"] = df.groupby(\"thinking\")[\"reward\"].apply(iqr)\n",
    "\n",
    "        next_generations = select_next(stats)\n",
    "\n",
    "        # 获取箱型图中得分异常思考步的索引\n",
    "        first_outlier_indices = []\n",
    "\n",
    "        for generation in next_generations:\n",
    "            data = Series(scores[generation])\n",
    "            Q1 = data.quantile(0.25)\n",
    "            IQR = iqr(scores[generation])\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            \n",
    "            # 找出异常值的布尔掩码\n",
    "            outliers_mask = (data < lower_bound)\n",
    "\n",
    "            # 获取异常值的索引\n",
    "            outlier_indices = data.index[outliers_mask].tolist()\n",
    "\n",
    "            # 第一个异常值索引\n",
    "            if len(outlier_indices) > 1:\n",
    "                first_outlier_indices.append(outlier_indices[1])\n",
    "            else:\n",
    "                first_outlier_indices.append(data.idxmin())\n",
    "\n",
    "        inputs = [\n",
    "            target_tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\n",
    "                        'role':'user',\n",
    "                        'content':'\\n\\n'.join(thinkings[generation].split('\\n\\n')[:idx+1])\n",
    "                    }\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=ADD_GENERATION_PROMPT\n",
    "            ) for generation, idx in zip(next_generations, first_outlier_indices)\n",
    "        ]\n",
    "\n",
    "        input_text = ['\\n\\n'.join(thinkings[generation].split('\\n\\n')[:idx+1]) for generation, idx in zip(next_generations, first_outlier_indices)]\n",
    "\n",
    "        gen_of_thinking += 1\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef04fc",
   "metadata": {},
   "source": [
    "1. 奖励分数贪婪搜索引导算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ea7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = target_tokenizer.apply_chat_template(\n",
    "    context,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "best_steps = []\n",
    "best_steps_with_score = []\n",
    "i = 0\n",
    "\n",
    "while(i < 50):\n",
    "\n",
    "    # 采样一步思考\n",
    "    output: list[RequestOutput] = target_model.generate(\n",
    "        input_str,\n",
    "        sampling_params=generating_params_1\n",
    "    )\n",
    "\n",
    "    thinking_steps = [item.text + \"\\n\\n\" for item in output[0].outputs]\n",
    "    context_with_rank_token = [input_str + thinking+ \"<extra_0>\" for thinking in thinking_steps]\n",
    "\n",
    "    rewards = reward_model.encode(context_with_rank_token)\n",
    "\n",
    "    scores = [reward.outputs.data.cpu().tolist()[0][1] for reward in rewards]\n",
    "\n",
    "    choosed_step = thinking_steps[scores.index(max(scores))]\n",
    "\n",
    "    best_steps_with_score.append(choosed_step[:-1] + f\"[{str(max(scores))}]\" + \"\\n\")\n",
    "\n",
    "    input_str += choosed_step\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"User: {context[0]['content']}\\n\\nAssistant: {''.join(best_steps_with_score)}\")\n",
    "\n",
    "    if output[0].outputs[scores.index(max(scores))].stop_reason == None:# or choosed_step.endswith(\"</think>\"):\n",
    "        break\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a413d4",
   "metadata": {},
   "source": [
    "2. 多阶段奖励分数束搜索引导算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260208d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一阶段采样\n",
    "\n",
    "output: list[RequestOutput] = target_model.generate(\n",
    "    input_str,\n",
    "    generating_params_1\n",
    ")\n",
    "\n",
    "# 第二阶段采样\n",
    "\n",
    "i = 0\n",
    "k = 3\n",
    "s = 10\n",
    "flag = True\n",
    "\n",
    "steps = answers[1].split(\"\\n\\n\")[:-l]\n",
    "steps_union = \"\\n\\n\".join(steps) + \"\\n\\n\"\n",
    "\n",
    "reward_context = [\n",
    "    {\"role\":\"user\", \"content\":question},\n",
    "    {\"role\":\"assistant\", \"content\":\"<extra_0>\".join(steps) + \"<extra_0>\"}\n",
    "]\n",
    "\n",
    "input_reward = reward_tokenizer.apply_chat_template(\n",
    "    reward_context,\n",
    "    add_generation_prompt=False,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# 采样思考\n",
    "outputs: list[RequestOutput] = target_model.generate(\n",
    "    input_prefix + steps_union,\n",
    "    sampling_params=generating_params_1\n",
    ")\n",
    "\n",
    "next_steps = [item.text for item in outputs[0].outputs]\n",
    "\n",
    "# 对采样的思考打分\n",
    "rewards = reward_model.encode([input_reward + next_step + \"<extra_0>\" for next_step in next_steps])\n",
    "\n",
    "# 取得分最高的 3 个思考\n",
    "beam = [(reward.outputs.data.cpu().tolist()[-1][1], next_step) for reward, next_step in zip(rewards, next_steps)]\n",
    "beam.sort(key=lambda x: x[0], reverse=True)\n",
    "beam = beam[:3]\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f\"User: {question}\\n\\nAssistant:\\n{'\\n--------------------------------------------------------------------------------\\n'.join(['Sequence ' + str(j) + \"\\n\\n\" + steps_union + b[1] + '\\n\\nBeam Search: [' + str(b[0]) + ']' for j, b in enumerate(beam)])}\")\n",
    "\n",
    "# 将 k 个思考加入序列\n",
    "inputs_target = [input_prefix + steps_union + step + \"\\n\\n\" for _, step in beam]\n",
    "\n",
    "# Beam Search\n",
    "while(flag and i < 10):\n",
    "\n",
    "    beam_k_sqr = []\n",
    "\n",
    "    # 对 k 个输入进行采样\n",
    "    outputs: list[RequestOutput] = target_model.generate(\n",
    "        inputs_target,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "\n",
    "    for j, output in enumerate(outputs):\n",
    "        next_steps = [item.text for item in output.outputs]\n",
    "\n",
    "        # 对采样的思考打分\n",
    "        rewards = reward_model.encode([input_reward + next_step + \"<extra_0>\" for next_step in next_steps])\n",
    "\n",
    "        # 取得分最高的 3 个思考\n",
    "        b = [(reward.outputs.data.cpu().tolist()[0][1], next_step) for reward, next_step in zip(rewards, next_steps)]\n",
    "        b.sort(key=lambda x: x[0], reverse=True)\n",
    "        b = b[:3]\n",
    "\n",
    "        # 扩展概率\n",
    "        for score, step in b:\n",
    "            beam_k_sqr.append((score * beam[j][0], beam[j][1] + \"\\n\\n\" + step))\n",
    "\n",
    "    beam_k_sqr.sort(key=lambda x: x[0], reverse=True)\n",
    "    beam = beam_k_sqr[:3]\n",
    "\n",
    "    if all(b[0] < 0.5 for b in beam):\n",
    "        beam[:] = [(score * 2, text) for score, text in beam]\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print(f\"User: {question}\\nAssistant:\\n{'\\n--------------------------------------------------------------------------------\\n'.join(['Sequence ' + str(j) + \"\\n\\n\" + steps_union + b[1] + '\\n\\nBeam Search: [' + str(b[0]) + ']' for j, b in enumerate(beam)])}\")\n",
    "\n",
    "    # 将 3 个思考加入序列\n",
    "    inputs_target = [input_prefix + steps_union + text + \"\\n\\n\" for _, text in beam]\n",
    "\n",
    "    flag = not all(\"</think>\" in b[1] for b in beam)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# 第三阶段采样\n",
    "\n",
    "results = []\n",
    "\n",
    "for input_str in input_strs:\n",
    "    output: list[RequestOutput] = target_model.generate(\n",
    "        input_str,\n",
    "        sampling_params=generating_params_2\n",
    "    )\n",
    "    results.append(output[0].outputs[0].text)# -------- Token 显示工具：将 \"Ġcapital\" 变为 \" capital\" --------\n",
    "def pretty_token(tok: str):\n",
    "    # GPT/Llama 字节级 BPE: Ġ 表示前导空格\n",
    "    if tok.startswith(\"Ġ\"):\n",
    "        return \" \" + tok[1:]\n",
    "    return tok\n",
    "\n",
    "# ===== 2. 参数配置 =====\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1,        # 不生成，只分析 prompt\n",
    "    temperature=1,\n",
    "    prompt_logprobs=1   # ⭐ 只返回 top-1\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "import random\n",
    "import json\n",
    "\n",
    "def counterfactual_gsm8k(data: dict) -> str:\n",
    "    ratios = [1/3, 0.5, 2, 3]\n",
    "    question = data[\"question\"]\n",
    "    correct_answer = int(data[\"answer\"].split('#### ')[-1])\n",
    "    incorrect_answer = str(random.sample([int(correct_answer * ratio) for ratio in ratios], 1)[0])\n",
    "    return f\"{question} Why the answer is {tokenizer.pad_token}?\"\n",
    "    # return f\"{question} Why {correct_answer} is incorrect?\"\n",
    "    # return f\"{question} There is a trap in the question, you won't figure out the correct answer at once.\"\n",
    "\n",
    "\n",
    "dataset = {\"gsm8k\": \"/root/project/dos/dataset/ours/gsm8k_sample.jsonl\"}\n",
    "\n",
    "with open(dataset[\"gsm8k\"], \"r\") as f:\n",
    "    counterfactuals = [counterfactual_gsm8k(json.loads(line)) for line in f.readlines()]\n",
    "\n",
    "print(f\"[Counterfactual Question]:\\n{counterfactuals[0]}\")\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{'role':'user','content':counterfactuals[0]}],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# ===== 3. 调用推理 =====\n",
    "outputs: list[RequestOutput] = llm.generate(prompt, sampling_params)\n",
    "req_out = outputs[0]\n",
    "\n",
    "prompt_token_ids = req_out.prompt_token_ids            # 长度 = L\n",
    "prompt_logprobs = req_out.prompt_logprobs              # 长度 = L - 1\n",
    "prompt_tokens = tokenizer.convert_ids_to_tokens(prompt_token_ids)\n",
    "# ===== 4. 打印表头 =====\n",
    "print(\"Idx | Prompt_Token     | Decoded   | Top1_Token       | Top1_Decoded | LogProb     | Prob     | Match\")\n",
    "print(\"----+------------------+-----------+-------------------+--------------+-------------+----------+-------\")\n",
    "\n",
    "probs_plot = []\n",
    "\n",
    "\n",
    "# ===== 5. 逐 token 对齐打印 =====\n",
    "for i, (tok_id, tok_str) in enumerate(zip(prompt_token_ids, prompt_tokens)):\n",
    "\n",
    "    decoded_tok = tokenizer.decode([tok_id])\n",
    "    pretty_tok = pretty_token(tok_str)\n",
    "\n",
    "    if i == 0:\n",
    "        # 第一个 token 无 logprob、无 top-1\n",
    "        print(f\"{i:3d} | {pretty_tok:16s} | {decoded_tok:9s} | {'N/A':17s} | {'N/A':12s} | {'N/A':11s} | {'N/A':8s} |   -  \")\n",
    "        probs_plot.append(float(\"nan\"))\n",
    "        continue\n",
    "\n",
    "    # 第 i 个 token 的 logprob 来自 prompt_logprobs[i-1]\n",
    "    step_dict = prompt_logprobs[i]   # dict[token_id → LogprobObj]\n",
    "\n",
    "    # 取 top-1 的 token_id\n",
    "    top1_id, lp_obj = max(step_dict.items(), key=lambda kv: kv[1].logprob)\n",
    "\n",
    "    top1_tok_str = tokenizer.convert_ids_to_tokens([top1_id])[0]\n",
    "    top1_decoded = tokenizer.decode([top1_id])\n",
    "\n",
    "    logprob = lp_obj.logprob\n",
    "    prob = math.exp(logprob)\n",
    "    probs_plot.append(prob)\n",
    "\n",
    "    # 比较 top-1 token 是否等于原本的 prompt token\n",
    "    match = (top1_decoded == decoded_tok)\n",
    "\n",
    "    print(\n",
    "        f\"{i:3d} | {pretty_tok:16s} | {decoded_tok:9s} | \"\n",
    "        f\"{pretty_token(top1_tok_str):17s} | {top1_decoded:12s} | \"\n",
    "        f\"{logprob:11.4f} | {prob:8.4f} | {'YES' if match else 'NO '}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ===== 6. 绘图 =====\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(len(probs_plot)), probs_plot, marker=\"o\")\n",
    "plt.xlabel(\"Token index\")\n",
    "plt.ylabel(\"Top-1 Probability of Actual Token\")\n",
    "plt.title(\"Prompt Token Probabilities (first token has no logprob)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()# 拿 tokenizer 解码 token，便于展示\n",
    "tokenizer = llm.get_tokenizer()\n",
    "\n",
    "# ===== 2. 配置 SamplingParams =====\n",
    "sampling_params = SamplingParams(\n",
    "    max_tokens=1,          # 这里只想算 prompt，不需要生成新 token\n",
    "    temperature=1,       # 随便设，这里不采样\n",
    "    prompt_logprobs=len(tokenizer)   # ⭐ 对每个 prompt token 返回全 vocab 的 logprobs\n",
    ")\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    [{\"role\":\"user\",\"content\":counterfactuals[0]}],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "# ===== 3. 调用 vLLM 推理 =====\n",
    "outputs: list[RequestOutput] = llm.generate(prompt, sampling_params)\n",
    "req_out = outputs[0]  # 只有一个 prompt，所以取第 0 个\n",
    "\n",
    "prompt_token_ids = req_out.prompt_token_ids         # list[int]\n",
    "prompt_logprobs = req_out.prompt_logprobs           # list[dict[token_id -> LogprobObj]]\n",
    "prompt_tokens = tokenizer.convert_ids_to_tokens(prompt_token_ids)\n",
    "\n",
    "# ===== 4. 对齐打印：每个 token 的 logprob & prob =====\n",
    "print(\"Idx | Token                     |   LogProb    |    Prob\")\n",
    "print(\"----+---------------------------+-------------+-----------\")\n",
    "\n",
    "probs_for_plot = []\n",
    "\n",
    "for i, (tok_id, tok_str) in enumerate(zip(prompt_token_ids, prompt_tokens)):\n",
    "    # 第一个 token 没有对应的 logprob\n",
    "    if i == 0:\n",
    "        logprob = float(\"nan\")\n",
    "        prob = float(\"nan\")\n",
    "        continue\n",
    "\n",
    "    # 第 i 个 token 的 logprob 来自 prompt_logprobs[i-1]\n",
    "    step_dict = prompt_logprobs[i]  # dict[token_id -> LogprobObj]\n",
    "    lp_obj = step_dict.get(tok_id)\n",
    "\n",
    "    if lp_obj is None:\n",
    "        # 说明该 token 不在返回的 top-K 里（如果你用的是 prompt_logprobs=K）\n",
    "        logprob = float(\"nan\")\n",
    "        prob = float(\"nan\")\n",
    "    else:\n",
    "        logprob = lp_obj.logprob\n",
    "        prob = math.exp(logprob)\n",
    "\n",
    "    probs_for_plot.append(prob)\n",
    "\n",
    "    display_tok = tok_str.replace(\"\\n\", \"\\\\n\")\n",
    "    if len(display_tok) > 25:\n",
    "        display_tok = display_tok[:22] + \"...\"\n",
    "\n",
    "    if math.isnan(logprob):\n",
    "        lp_str = \"   N/A\"\n",
    "        p_str = \"   N/A\"\n",
    "    else:\n",
    "        lp_str = f\"{logprob:11.4f}\"\n",
    "        p_str = f\"{prob:9.4f}\"\n",
    "\n",
    "    print(f\"{i:3d} | {display_tok:25s} | {lp_str} | {p_str}\")\n",
    "\n",
    "# ===== 5. 用 matplotlib 可视化每个 token 的 prob =====\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(range(len(probs_for_plot)), probs_for_plot, marker=\"o\")\n",
    "plt.xlabel(\"Token index in prompt\")\n",
    "plt.ylabel(\"P(actual token)\")\n",
    "plt.title(\"Prompt token probabilities\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 在 notebook 里直接 show；脚本环境可以改成 savefig\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 或者：\n",
    "# plt.savefig(\"prompt_token_probs.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
