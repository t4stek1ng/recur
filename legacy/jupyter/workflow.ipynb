{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b0534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############   函数    ###############\n",
    "\n",
    "# GSM8K数据集反事实化改写\n",
    "def counterfactual_gsm8k(data: dict) -> str:\n",
    "    ratios = [0.5, 1.5, 2]\n",
    "    question = data[\"question\"]\n",
    "    correct_answer = int(data[\"answer\"].split('#### ')[-1])\n",
    "    incorrect_answer = str(random.sample([int(correct_answer * ratio) for ratio in ratios], 1))\n",
    "    return f\"{question} Why the correct answer is: {incorrect_answer}?\"\n",
    "\n",
    "# MMLU数据集反事实化改写\n",
    "def counterfactual_mmlu(data: dict) -> str:\n",
    "    choices = ['A', 'B', 'C', 'D']\n",
    "    question = data[\"question\"]\n",
    "    choices.remove(data[\"answer\"])\n",
    "    incorrect_choice = random.sample(choices, 1)[0]\n",
    "    return f\"There is a multiple-choice question: {question}. The options are: A. {data[\"A\"]}, B. {data[\"B\"]}, C. {data[\"C\"]}, D. {data[\"D\"]}. Why the correct answer is: {incorrect_choice}. {data[incorrect_choice]}?\"\n",
    "\n",
    "# GPQA数据集反事实化改写\n",
    "def counterfactual_gpqa(data: dict) -> str:\n",
    "    question = data[\"question\"]\n",
    "    incorrect_answer = random.sample(data[\"incorrect answers\"], 1)[0]\n",
    "    return f\"{question} Why the correct answer is: {incorrect_answer}?\"\n",
    "\n",
    "\n",
    "def query(input: str, temperature: float, n: int, max_len: int) -> list[str] | str:\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_len,\n",
    "        n=n\n",
    "    )\n",
    "\n",
    "    prompt = target_tokenizer.apply_chat_template(\n",
    "        [{'role':'user','content':input}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    outputs: list[RequestOutput] = target_model.generate(\n",
    "        prompt,\n",
    "        sampling_params\n",
    "    )\n",
    "\n",
    "    if n == 1:\n",
    "        return outputs[0].outputs[0].text\n",
    "    else:\n",
    "        return [output.text for output in outputs[0].outputs]\n",
    "\n",
    "\n",
    "def combine_suffix(q: str, n: str, s: str) -> str:\n",
    "    prompt = target_tokenizer.apply_chat_template(\n",
    "        [{'role':'user','content':q}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    return prompt + n + s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e4728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#############  数据集  ##############\n",
    "\n",
    "dataset = {\n",
    "    \"gsm8k\": \"/root/project/dos/dataset/ours/gsm8k_sample.jsonl\",\n",
    "    \"mmlu\": [\n",
    "        \"/root/project/dos/dataset/ours/college_computer_science_sample.jsonl\",\n",
    "        \"/root/project/dos/dataset/ours/college_physics_sample.jsonl\",\n",
    "        \"/root/project/dos/dataset/ours/econometrics_sample.jsonl\",\n",
    "        \"/root/project/dos/dataset/ours/logical_fallacies_sample.jsonl\"\n",
    "    ],\n",
    "    \"GPQA\": \"/root/project/dos/dataset/ours/gpqa_sample.jsonl\"\n",
    "}\n",
    "\n",
    "counterfactuals = []\n",
    "\n",
    "with open(dataset[\"gsm8k\"], \"r\") as f:\n",
    "    counterfactuals.extend([counterfactual_gsm8k(json.loads(line)) for line in f.readlines()])\n",
    "\n",
    "for path in dataset[\"mmlu\"]:\n",
    "    with open(path, \"r\") as f:\n",
    "        counterfactuals.extend([counterfactual_mmlu(json.loads(line)) for line in f.readlines()])\n",
    "\n",
    "with open(dataset[\"GPQA\"], \"r\") as f:\n",
    "    counterfactuals.extend([counterfactual_gpqa(json.loads(line)) for line in f.readlines()])\n",
    "\n",
    "counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from googletrans import Translator\n",
    "\n",
    "def en_to_zh(text: str) -> str:\n",
    "    translator = Translator()\n",
    "    loop = asyncio.get_event_loop()\n",
    "    result = loop.run_until_complete(translator.translate(text, src='en', dest='zh-cn'))\n",
    "    return result.text\n",
    "\n",
    "print(en_to_zh(\"Hello, how are you today?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imitate overthinking\n",
    "deriative_overthinking = []\n",
    "for line in lines[end_idx+2:end_idx+3]:\n",
    "    for word in head:\n",
    "        if word in line:\n",
    "            for other_word in head:\n",
    "                if other_word != word:\n",
    "                    new_line = line.replace(word, other_word)\n",
    "                    deriative_overthinking.append(new_line)\n",
    "\n",
    "    for n in noun:\n",
    "        if n in line:\n",
    "            for other_noun in noun:\n",
    "                if other_noun != n:\n",
    "                    new_line = line.replace(n, other_noun)\n",
    "                    deriative_overthinking.append(new_line)\n",
    "\n",
    "    for v in verb:\n",
    "        if v in line:\n",
    "            for other_verb in verb:\n",
    "                if other_verb != v:\n",
    "                    new_line = line.replace(v, other_verb)\n",
    "                    deriative_overthinking.append(new_line)\n",
    "\n",
    "    for s in short:\n",
    "        if s in line:\n",
    "            for other_short in short:\n",
    "                if other_short != s:\n",
    "                    new_line = line.replace(s, other_short)\n",
    "                    deriative_overthinking.append(new_line)\n",
    "\n",
    "\n",
    "suffix = \"\\n\\n\".join(deriative_overthinking) + \"\\n\\n\"\n",
    "input_str = combine_suffix(counterfactual, normal_thinking, suffix)\n",
    "print(input_str+\"\\n\\n\")\n",
    "\n",
    "outputs_1 = target_model.generate(\n",
    "    input_str,\n",
    "    SamplingParams(\n",
    "        temperature=config[\"temperature\"],\n",
    "        max_tokens=config[\"max_model_len\"],\n",
    "        n=config[\"n\"]\n",
    "    )\n",
    ")\n",
    "overthink_outputs_1 = [output for output in outputs if \"</think>\" not in output] # 收集那些达到模型长度上限的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec168993",
   "metadata": {},
   "source": [
    "- 字典构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/root/project/dos/words_new.json\", \"r\") as f:\n",
    "    lib = json.loads(f.read())\n",
    "\n",
    "PH1 = '{head}'\n",
    "PH2 = '{error}'\n",
    "PH3 = '{solution}'\n",
    "\n",
    "head = lib['head']\n",
    "error = lib['error']\n",
    "solution = lib['solution']\n",
    "# 先按长度从长到短排序，避免 \"New York\" 被 \"New\" 先匹配掉\n",
    "sorted_head = sorted(head, key=len, reverse=True)\n",
    "sorted_error = sorted(error, key=len, reverse=True)\n",
    "sorted_solution = sorted(solution, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 re.escape 转义特殊字符，避免它们被当成正则元字符\n",
    "pattern_head = \"|\".join(re.escape(w) for w in sorted_head)\n",
    "pattern_head = r\"^(?:\" + pattern_head + r\")\"\n",
    "pattern_error = \"|\".join(re.escape(w) for w in sorted_error)\n",
    "pattern_solution = \"|\".join(re.escape(w) for w in sorted_solution)\n",
    "\n",
    "head_re = re.compile(pattern_head)\n",
    "error_re = re.compile(pattern_error)\n",
    "solution_re = re.compile(pattern_solution)\n",
    "\n",
    "\n",
    "def replace_with_placeholders(text: str) -> str:\n",
    "    # 按顺序对三个列表依次替换\n",
    "    # 如果同一段文本可能同时匹配多个列表，先后顺序会影响结果\n",
    "    text = head_re.sub(PH1, text)\n",
    "    text = error_re.sub(PH2, text)\n",
    "    text = solution_re.sub(PH3, text)\n",
    "    return text\n",
    "\n",
    "def imitate_overthink(overthink_steps: list[str]) -> list[str]:\n",
    "    overthink_template = []\n",
    "    for step in overthink_steps:\n",
    "        overthink_template.append(replace_with_placeholders(step))\n",
    "    \n",
    "    return overthink_template\n",
    "\n",
    "temp = imitate_overthink(overthink_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77040039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取模式\n",
    "head = set()\n",
    "error = set()\n",
    "solution = set()\n",
    "\n",
    "for i in range(1):\n",
    "\n",
    "    try:\n",
    "        with open(f\"/root/project/dos/data/{str(i)}_overthink.txt\", \"r\") as f:\n",
    "            overthinkings = f.read().split(f\"\\n{'-'*50}\\n\")\n",
    "            thinking_steps = f.read().split(f\"\\n{'-'*50}\\n\")[0].split(\"\\n\\n\")\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    for thinking in overthinkings:\n",
    "        overthinking_steps = thinking.split(\"\\n\\n\")\n",
    "        for text in overthinking_steps:\n",
    "            ## [head]\n",
    "            match = re.match(r\"^([A-Za-z]+),\", text)\n",
    "            if match:\n",
    "                head.add(match.group(1))\n",
    "\n",
    "            ## [error]\n",
    "            pattern = re.compile(r\"\\b(I must|maybe I)[^.,]*\", re.IGNORECASE)\n",
    "            m = pattern.search(text)\n",
    "            if m:\n",
    "                error.add(m.group(0))\n",
    "            ## [solution]\n",
    "            pattern = re.compile(r\"\\b(Let me)[^.,;:!?()\\[\\]\\\"'…]*\", re.IGNORECASE)\n",
    "            match = pattern.match(text)\n",
    "            if match:\n",
    "                solution.add(match.group(0).strip())\n",
    "\n",
    "print(head)\n",
    "print(error)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ecf49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def overthinking_gen(overthink_template: list[str]) -> list[str]:\n",
    "    overthinking = []\n",
    "    for step in overthink_template:\n",
    "        overthinking.append(step.format(**{\"head\":random.sample(sorted(head),1)[0], \"error\":random.sample(sorted(error),1)[0], \"solution\":random.sample(sorted(solution),1)[0]}))\n",
    "\n",
    "    return overthinking\n",
    "\n",
    "rewrited_overthinkings = []\n",
    "for _ in range(3):\n",
    "    rewrited_overthinkings.append(overthinking_gen(temp))\n",
    "\n",
    "rewrited_overthinkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertion = \"\\n\\n\".join([\"\\n\\n\".join([step for step in steps]) for steps in rewrited_overthinkings]) + \"\\n\\n\"\n",
    "print(insertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bba06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_suffix(q: str, original: str, s: str) -> str:\n",
    "    prompt = target_tokenizer.apply_chat_template(\n",
    "        [{'role':'user','content':q}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    return prompt + original + s\n",
    "\n",
    "original_thinking = \"\\n\\n\".join(thinking_steps[:j+1+overthink_offset]) + \"\\n\\n\"\n",
    "input_str = combine_suffix(counterfactual, original_thinking, insertion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
