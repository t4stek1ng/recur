{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "458d85e2",
   "metadata": {},
   "source": [
    "- 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388fc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from vllm import LLM, SamplingParams, RequestOutput\n",
    "from vllm.sequence import PromptLogprobs, SampleLogprobs, Logprob\n",
    "\n",
    "config = json.load(open('../config.json', 'r'))\n",
    "os.environ['RANK'] = '0'\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = config['gpu']\n",
    "temperature = config[\"temperature\"]\n",
    "max_model_len = config[\"max_model_len\"]\n",
    "max_reflect = config[\"max_reflect\"]\n",
    "logprobs = config[\"logprobs\"]\n",
    "k=3\n",
    "\n",
    "llm = LLM(\n",
    "    model=config['models'][config['model_id']],\n",
    "    task=\"generate\",\n",
    "    max_model_len=config['max_model_len'],\n",
    "    gpu_memory_utilization=config['gpu_memory_utilization'],\n",
    "    enable_chunked_prefill=False\n",
    ")\n",
    "\n",
    "tokenizer = llm.get_tokenizer()\n",
    "over_reflect_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "prompt_thinking_step_entropy_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    max_tokens=1,\n",
    "    prompt_logprobs=logprobs\n",
    ")\n",
    "\n",
    "min_thinking_step_entropy_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    max_tokens=8192,\n",
    "    logprobs=logprobs,\n",
    "    stop=\"\\n\\n\",\n",
    "    include_stop_str_in_output=True\n",
    ")\n",
    "\n",
    "entropy_down_1_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    max_tokens=1,\n",
    "    logprobs=k,\n",
    ")\n",
    "\n",
    "entropy_down_2_params = SamplingParams(\n",
    "    temperature=1,\n",
    "    max_tokens=1,\n",
    "    logprobs=logprobs\n",
    ")\n",
    "\n",
    "loop_generation_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=8192\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df8f4ad",
   "metadata": {},
   "source": [
    "- 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df3063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# gsm8k\n",
    "\n",
    "with open(\"/root/project/dos/dataset/ours/gsm8k_sample.jsonl\", \"r\") as f:\n",
    "    gsm8k_dataset: list[dict[str, list[str]] | str | int] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for i, data in enumerate(gsm8k_dataset):\n",
    "    gsm8k_dataset[i] = {\n",
    "        \"id\": i,\n",
    "        \"question\": data[\"question\"],\n",
    "        \"options\": None,\n",
    "        \"option\": None,\n",
    "        \"answer\": data[\"answer\"].split(\"\\n#### \")[1],\n",
    "        \"incorrect\": []\n",
    "    }\n",
    "\n",
    "# mmlu\n",
    "OPTIONS = {\"A\", \"B\", \"C\", \"D\"}\n",
    "\n",
    "with open(\"/root/project/dos/dataset/ours/college_physics_sample.jsonl\", \"r\") as f:\n",
    "    mmlu_physics_dataset: list[dict[str, list[str]] | str | int] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for i, data in enumerate(mmlu_physics_dataset):\n",
    "    answer_option = data[\"answer\"]\n",
    "    answer = data[answer_option]\n",
    "    mmlu_physics_dataset[i] = {\n",
    "        \"id\": i,\n",
    "        \"question\": data[\"question\"],\n",
    "        \"options\": {\n",
    "            \"A\": data[\"A\"],\n",
    "            \"B\": data[\"B\"],\n",
    "            \"C\": data[\"C\"],\n",
    "            \"D\": data[\"D\"]\n",
    "        },\n",
    "        \"option\": answer_option,\n",
    "        \"answer\": answer,\n",
    "        \"incorrect\": [data[option] for option in OPTIONS if option != answer_option]\n",
    "    }\n",
    "\n",
    "with open(\"/root/project/dos/dataset/ours/college_computer_science_sample.jsonl\", \"r\") as f:\n",
    "    mmlu_cs_dataset: list[dict[str, list[str]] | str | int] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for i, data in enumerate(mmlu_cs_dataset):\n",
    "    answer_option = data[\"answer\"]\n",
    "    answer = data[answer_option]\n",
    "    mmlu_cs_dataset[i] = {\n",
    "        \"id\": i,\n",
    "        \"question\": data[\"question\"],\n",
    "        \"options\": {\n",
    "            \"A\": data[\"A\"],\n",
    "            \"B\": data[\"B\"],\n",
    "            \"C\": data[\"C\"],\n",
    "            \"D\": data[\"D\"]\n",
    "        },\n",
    "        \"option\": answer_option,\n",
    "        \"answer\": answer,\n",
    "        \"incorrect\": [data[option] for option in OPTIONS if option != answer_option]\n",
    "    }\n",
    "\n",
    "with open(\"/root/project/dos/dataset/ours/econometrics_sample.jsonl\", \"r\") as f:\n",
    "    mmlu_econometrics_dataset: list[dict[str, list[str]] | str | int] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for i, data in enumerate(mmlu_econometrics_dataset):\n",
    "    answer_option = data[\"answer\"]\n",
    "    answer = data[answer_option]\n",
    "    mmlu_econometrics_dataset[i] = {\n",
    "        \"id\": i,\n",
    "        \"question\": data[\"question\"],\n",
    "        \"options\": {\n",
    "            \"A\": data[\"A\"],\n",
    "            \"B\": data[\"B\"],\n",
    "            \"C\": data[\"C\"],\n",
    "            \"D\": data[\"D\"]\n",
    "        },\n",
    "        \"option\": answer_option,\n",
    "        \"answer\": answer,\n",
    "        \"incorrect\": [data[option] for option in OPTIONS if option != answer_option]\n",
    "    }\n",
    "\n",
    "with open(\"/root/project/dos/dataset/ours/logical_fallacies_sample.jsonl\", \"r\") as f:\n",
    "    mmlu_logical_dataset: list[dict[str, list[str]] | str | int] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for i, data in enumerate(mmlu_logical_dataset):\n",
    "    answer_option = data[\"answer\"]\n",
    "    answer = data[answer_option]\n",
    "    mmlu_logical_dataset[i] = {\n",
    "        \"id\": i,\n",
    "        \"question\": data[\"question\"],\n",
    "        \"options\": {\n",
    "            \"A\": data[\"A\"],\n",
    "            \"B\": data[\"B\"],\n",
    "            \"C\": data[\"C\"],\n",
    "            \"D\": data[\"D\"]\n",
    "        },\n",
    "        \"option\": answer_option,\n",
    "        \"answer\": answer,\n",
    "        \"incorrect\": [data[option] for option in OPTIONS if option != answer_option]\n",
    "    }\n",
    "\n",
    "# gpqa\n",
    "\n",
    "with open(\"/root/project/dos/dataset/ours/gpqa_sample.jsonl\", \"r\") as f:\n",
    "    gpqa_dataset: list[dict[str, list[str]] | str | int] = [json.loads(line) for line in f.readlines()]\n",
    "\n",
    "for i, data in enumerate(gpqa_dataset):\n",
    "    gpqa_dataset[i] = {\n",
    "        \"id\": i,\n",
    "        \"question\": data[\"question\"],\n",
    "        \"options\": None,\n",
    "        \"option\": None,\n",
    "        \"answer\": data[\"answer\"].strip(),\n",
    "        \"incorrect\": [incorrect_answer.strip() for incorrect_answer in data[\"incorrect answers\"]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008922dc",
   "metadata": {},
   "source": [
    "- 反事实提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec7fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def counterfactual_prompting(data: list[dict[str, list[str]] | str | int], mode: str) -> str:\n",
    "    question = data[\"question\"]\n",
    "    option = data[\"option\"]\n",
    "    answer = data[\"answer\"]\n",
    "    incorrect_answers = data[\"incorrect\"]\n",
    "\n",
    "    if option == None:\n",
    "        if incorrect_answers == []: # gsm8k\n",
    "            incorrect_answer = None\n",
    "            if mode == \"wrong\":\n",
    "                return f\"{question} Why the answer is {str(int(int(answer) * (0.5 + random.random())))}?\"\n",
    "            elif mode == \"deny\":\n",
    "                return f\"{question} Why {answer} is incorrect?\"\n",
    "            elif mode == \"suspect\":\n",
    "                return f\"{question} There is a trap in the question, you won't figure out the correct answer at once.\"\n",
    "        else: # gpqa\n",
    "            incorrect_answer = random.sample(incorrect_answers, 1)[0]\n",
    "            if mode == \"wrong\":\n",
    "                return f\"{question} Why the answer is {incorrect_answer}?\"\n",
    "            elif mode == \"deny\":\n",
    "                return f\"{question} Why {answer} is incorrect?\"\n",
    "            elif mode == \"suspect\":\n",
    "                return f\"{question} There is a trap in the question, you won't figure out the correct answer at once.\"\n",
    "    else: # mmlu\n",
    "        incorrect_option = random.sample([opt for opt in OPTIONS if opt != option])\n",
    "        incorrect_answer = data[\"options\"][incorrect_option]\n",
    "        if mode == \"wrong\":\n",
    "            return f\"{question} A. {data[\"options\"][\"A\"]}, B. {data[\"options\"][\"B\"]}, C. {data[\"options\"][\"C\"]}, D. {data[\"options\"][\"D\"]}. Why the answer is {incorrect_option}. {incorrect_answer}?\"\n",
    "        elif mode == \"deny\":\n",
    "            return f\"{question} A. {data[\"options\"][\"A\"]}, B. {data[\"options\"][\"B\"]}, C. {data[\"options\"][\"C\"]}, D. {data[\"options\"][\"D\"]}. Why {option}. {answer} is incorrect?\"\n",
    "        elif mode == \"suspect\":\n",
    "            return f\"{question} A. {data[\"options\"][\"A\"]}, B. {data[\"options\"][\"B\"]}, C. {data[\"options\"][\"C\"]}, D. {data[\"options\"][\"D\"]}. There is a trap in the question, you won't figure out the correct answer at once.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3c489f",
   "metadata": {},
   "source": [
    "- 最小思考步熵搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a642fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from vllm.sequence import PromptLogprobs, SampleLogprobs, Logprob\n",
    "\n",
    "# 循环结束条件：生成的思考步的熵小于熵的估算误差上界\n",
    "top_p = 0.99\n",
    "\n",
    "# ---------- 3. tail 概率质量 ----------\n",
    "q = 1 - top_p  # tail 质量\n",
    "\n",
    "# ---------- 4. tail token 数量 ----------\n",
    "M = tokenizer.vocab_size\n",
    "\n",
    "# ---------- 5. tail 熵上界 ----------\n",
    "entropy_inaccuracy: float = -q * math.log(q) + q * math.log(M)\n",
    "\n",
    "def trunc_entropy_with_error_bounds(\n",
    "    step_logprobs: dict[int, Logprob],\n",
    "    top_p: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    根据 step_logprobs 计算：\n",
    "    - 不重新归一化的截断熵 H_trunc\n",
    "    - tail 概率质量 q\n",
    "    - 真实熵的下界 H_min\n",
    "    - 真实熵的上界 H_max\n",
    "\n",
    "    返回（单位：nats）：\n",
    "        H_trunc, q, H_min, H_max\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- 1. 从 logprobs 得到概率 ----------\n",
    "    probs = []\n",
    "    prob_sum = 0\n",
    "    \n",
    "    for lp in step_logprobs.values():\n",
    "        prob = math.exp(lp.logprob)\n",
    "        probs.append(prob)\n",
    "        prob_sum += prob\n",
    "        \n",
    "        if prob_sum > top_p:\n",
    "            break\n",
    "\n",
    "    # ---------- 2. 计算截断熵 ----------\n",
    "    # H_trunc = - sum_{i in S} p_i log p_i\n",
    "    H_trunc = 0.0\n",
    "    for p in probs:\n",
    "        H_trunc -= p * math.log(p)\n",
    "\n",
    "    return H_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed00834",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = {\"wrong\", \"deny\", \"suspect\"}\n",
    "for mode in MODE:\n",
    "    print(f\"Counterfactual Mode: {mode}\")\n",
    "    data = gsm8k_dataset[0]\n",
    "    option = data[\"option\"]\n",
    "    answer = data[\"answer\"]\n",
    "    content = counterfactual_prompting(data, mode)\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\":\"user\", \"content\":content}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    over_reflect_flag = False\n",
    "    for id in range(3):\n",
    "        print(f\"ID: {id}\")\n",
    "        outputs: list[RequestOutput] = llm.generate(\n",
    "            prompt,\n",
    "            over_reflect_params,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "\n",
    "        output = outputs[0]\n",
    "        text = output.outputs[0].text\n",
    "        over_reflect_begin_idx = over_reflect_check(text, option, answer)\n",
    "        if over_reflect_begin_idx:\n",
    "            over_reflect_flag = True\n",
    "            break\n",
    "\n",
    "    if over_reflect_flag:\n",
    "        break\n",
    "\n",
    "over_reflect_steps = text.split(\"\\n\\n\")[:over_reflect_begin_idx+1]\n",
    "over_reflect_output = \"\\n\\n\".join(over_reflect_steps) + \"\\n\\n\"\n",
    "over_reflect_prompt = prompt + over_reflect_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6057b6",
   "metadata": {},
   "source": [
    "- 采样方法1：降低每一步平均熵 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 over_reflect_prompt 中最后一步思考的熵\n",
    "outputs = llm.generate(\n",
    "    over_reflect_prompt,\n",
    "    prompt_thinking_step_entropy_params,\n",
    "    use_tqdm=False\n",
    ")\n",
    "\n",
    "total_entropy = 0\n",
    "\n",
    "# over_reflect_prompt_last_step = text.split(\"\\n\\n\")[over_reflect_begin_idx]+\"\\n\\n\"\n",
    "over_reflect_prompt_last_step = over_reflect_prompt.split(\"\\n\\n\")[-2] + \"\\n\\n\"\n",
    "\n",
    "output = outputs[0]\n",
    "prompt_logprobs: PromptLogprobs = output.prompt_logprobs\n",
    "over_reflect_prompt_last_step_tokens = tokenizer.tokenize(over_reflect_prompt_last_step)\n",
    "over_reflect_prompt_last_step_ids = tokenizer.convert_tokens_to_ids(over_reflect_prompt_last_step_tokens)\n",
    "over_reflect_prompt_last_step_len = len(over_reflect_prompt_last_step_tokens)\n",
    "over_reflect_prompt_last_step_logprobs = prompt_logprobs[-(over_reflect_prompt_last_step_len):] # 得到了最后一步的 logprobs\n",
    "\n",
    "entropy_sum = 0\n",
    "for step_logprobs in over_reflect_prompt_last_step_logprobs:\n",
    "    entropy = trunc_entropy_with_error_bounds(step_logprobs, 0.99)\n",
    "    entropy_sum += entropy\n",
    "\n",
    "count = 1\n",
    "\n",
    "last_step_avg_entropy = entropy_sum / over_reflect_prompt_last_step_len\n",
    "total_entropy += last_step_avg_entropy\n",
    "\n",
    "avg_entropy = total_entropy / count\n",
    "\n",
    "print(f\"Inaccuracy: {entropy_inaccuracy}, Last thinking step average entropy: {last_step_avg_entropy}\")\n",
    "\n",
    "while(True):\n",
    "    while(True):\n",
    "        outputs = llm.generate(\n",
    "            over_reflect_prompt,\n",
    "            min_thinking_step_entropy_params,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "\n",
    "        next_step_logprobs = outputs[0].outputs[0].logprobs\n",
    "        entropy_sum = 0\n",
    "        for step_logprobs in next_step_logprobs:\n",
    "            entropy = trunc_entropy_with_error_bounds(step_logprobs, 0.99)\n",
    "            entropy_sum += entropy\n",
    "\n",
    "        next_step_avg_entropy = entropy_sum / len(next_step_logprobs)\n",
    "        if (total_entropy + next_step_avg_entropy) / (count + 1) < avg_entropy:\n",
    "            print(f\"New thinking step average entropy: {next_step_avg_entropy}\")\n",
    "            print(f\"Thinking: {outputs[0].outputs[0].text}\")\n",
    "            break\n",
    "\n",
    "    over_reflect_prompt += outputs[0].outputs[0].text\n",
    "    total_entropy += next_step_avg_entropy\n",
    "    count += 1\n",
    "    avg_entropy = total_entropy / count\n",
    "\n",
    "    if entropy_inaccuracy > next_step_avg_entropy:\n",
    "        print(f\"Input: {over_reflect_prompt}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cf5a9",
   "metadata": {},
   "source": [
    "- 采样方法2：根据概率选择下一步熵最低的token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff146947",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 0.99/entropy_inaccuracy\n",
    "print(f\"Prob/Entropy: {bound}\")\n",
    "eot_token_id = tokenizer.encode(\"</think>\")[0]\n",
    "\n",
    "# 比较 top-k 个 token 后续预测的熵值\n",
    "outputs = llm.generate(\n",
    "    over_reflect_prompt,\n",
    "    entropy_down_1_params,\n",
    "    use_tqdm=False\n",
    ")\n",
    "\n",
    "candidate_token_logprobs = outputs[0].outputs[0].logprobs[0].items()\n",
    "\n",
    "while(True):\n",
    "    data: list[tuple[int, str, float, dict[int, Logprob]]] = []\n",
    "    for id, logprob in candidate_token_logprobs:\n",
    "        token = logprob.decoded_token\n",
    "        prob = math.exp(logprob.logprob)\n",
    "        over_reflect_prompt_temp = over_reflect_prompt + token\n",
    "\n",
    "        outputs = llm.generate(\n",
    "            over_reflect_prompt_temp,\n",
    "            entropy_down_2_params,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "\n",
    "        next_step_logprobs = outputs[0].outputs[0].logprobs[0]\n",
    "        entropy = trunc_entropy_with_error_bounds(next_step_logprobs, 0.99)\n",
    "        data.append((id, token, prob/entropy, next_step_logprobs))\n",
    "\n",
    "    data.sort(key=lambda x: x[2], reverse=True)\n",
    "    next_token_id = data[0][0]\n",
    "    next_token = data[0][1]\n",
    "    over_reflect_prompt += next_token\n",
    "    next_prob_entropy = data[0][2]\n",
    "    print(f\"Next token: '{next_token}'. Next prob/entropy: {next_prob_entropy}.\\n\")\n",
    "    candidate_token_logprobs = sorted(data[0][3].items(), key=lambda x: x[1].rank)[:5]\n",
    "    if next_token_id == tokenizer.eos_token_id or next_token_id == eot_token_id:\n",
    "        break\n",
    "\n",
    "    if loop_check(over_reflect_prompt):\n",
    "        break\n",
    "\n",
    "print(f\"Prompt: {over_reflect_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e34c0f",
   "metadata": {},
   "source": [
    "- 测试生成循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95931284",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_generation_params = SamplingParams(\n",
    "    temperature=0.2,\n",
    "    max_tokens=8192\n",
    ")\n",
    "\n",
    "outputs = llm.generate(\n",
    "    over_reflect_prompt,\n",
    "    loop_generation_params,\n",
    "    use_tqdm=False\n",
    ")\n",
    "\n",
    "print(outputs[0].outputs[0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
